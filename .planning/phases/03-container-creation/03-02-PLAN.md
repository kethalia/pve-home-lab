---
phase: 03-container-creation
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - apps/dashboard/src/workers/container-creation.ts
autonomous: true

must_haves:
  truths:
    - "Container creation jobs are processed within seconds of submission"
    - "Worker picks up jobs from container-creation queue and executes the 5-phase pipeline"
    - "Each pipeline phase publishes progress events to Redis Pub/Sub channel container:<id>:progress"
    - "Each pipeline phase persists ContainerEvent records in the database"
    - "Config-manager is deployed to the container, providing directory structure, config.env, and systemd service"
    - "Container lifecycle transitions: creating → ready on success, creating → error on failure"
    - "Discovered services and credentials are saved as ContainerService records"
    - "Worker handles graceful shutdown on SIGTERM/SIGINT"
  artifacts:
    - path: "apps/dashboard/src/workers/container-creation.ts"
      provides: "BullMQ Worker process with 5-phase container creation pipeline"
      contains: "new Worker"
      min_lines: 150
  key_links:
    - from: "apps/dashboard/src/workers/container-creation.ts"
      to: "apps/dashboard/src/lib/proxmox/containers.ts"
      via: "Calls createContainer, startContainer via Proxmox API"
      pattern: "createContainer|startContainer"
    - from: "apps/dashboard/src/workers/container-creation.ts"
      to: "apps/dashboard/src/lib/ssh.ts"
      via: "Uses SSHSession for post-creation deployment"
      pattern: "SSHSession|connectWithRetry"
    - from: "apps/dashboard/src/workers/container-creation.ts"
      to: "apps/dashboard/src/lib/queue/container-creation.ts"
      via: "Imports ContainerJobData type and getProgressChannel"
      pattern: "ContainerJobData|getProgressChannel"
    - from: "apps/dashboard/src/workers/container-creation.ts"
      to: "apps/dashboard/src/lib/db.ts"
      via: "Updates Container lifecycle and creates ContainerEvent records"
      pattern: "prisma\\.container\\.(update|findUnique)|prisma\\.containerEvent\\.create"
---

<objective>
Build the BullMQ worker process that executes the 5-phase container creation pipeline: Create → Start → Deploy config-manager + files (SSH) → Run config-manager + scripts (SSH) → Discover services + Finalize. The worker runs as a standalone process, publishes real-time progress via Redis Pub/Sub, and persists events in the database.

Purpose: This is the backend engine that turns a wizard submission into a running, configured container. It deploys the config-manager infrastructure (directories, config.env, systemd service), runs template scripts, discovers running services and credentials, and creates ContainerService records.

Output: A worker file at `src/workers/container-creation.ts` runnable via `tsx --watch`.
</objective>

<execution_context>
@/home/coder/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/coder/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-container-creation/03-RESEARCH.md
@.planning/phases/03-container-creation/03-01-SUMMARY.md

@apps/dashboard/src/lib/proxmox/containers.ts
@apps/dashboard/src/lib/proxmox/tasks.ts
@apps/dashboard/src/lib/proxmox/index.ts
@apps/dashboard/src/lib/ssh.ts
@apps/dashboard/src/lib/queue/container-creation.ts
@apps/dashboard/src/lib/redis.ts
@apps/dashboard/src/lib/db.ts
@apps/dashboard/src/lib/encryption.ts
@apps/dashboard/prisma/schema.prisma
</context>

<tasks>

<task type="auto">
  <name>Task 1: Container creation worker with 5-phase pipeline</name>
  <files>
    apps/dashboard/src/workers/container-creation.ts
  </files>
  <action>
Create `src/workers/container-creation.ts` — the standalone BullMQ worker process.

**File structure:**

1. **Imports:**

   ```ts
   import { Worker, Job } from "bullmq";
   import Redis from "ioredis";
   import {
     type ContainerJobData,
     type ContainerJobResult,
     type ContainerProgressEvent,
     getProgressChannel,
   } from "../lib/queue/container-creation";
   import {
     DatabaseService,
     ContainerLifecycle,
     EventType,
     ServiceType,
     ServiceStatus,
     prisma,
   } from "../lib/db";
   import { createProxmoxClientFromNode } from "../lib/proxmox";
   import { createContainer, startContainer } from "../lib/proxmox/containers";
   import { waitForTask } from "../lib/proxmox/tasks";
   import { connectWithRetry, type SSHSession } from "../lib/ssh";
   import { decrypt } from "../lib/encryption";
   ```

2. **Redis connections:** Create TWO separate Redis connections:

   ```ts
   const workerConnection = new Redis(process.env.REDIS_URL!, {
     maxRetriesPerRequest: null, // REQUIRED for BullMQ Worker
   });
   const publisher = new Redis(process.env.REDIS_URL!);
   ```

3. **Progress helper:**

   ```ts
   async function publishProgress(
     containerId: string,
     event: Omit<ContainerProgressEvent, "timestamp">,
   ): Promise<void> {
     const fullEvent: ContainerProgressEvent = {
       ...event,
       timestamp: new Date().toISOString(),
     };
     // Publish to Redis for SSE subscribers
     await publisher.publish(
       getProgressChannel(containerId),
       JSON.stringify(fullEvent),
     );
     // Persist to database for late subscribers / audit trail
     await DatabaseService.createContainerEvent({
       containerId,
       type:
         event.type === "complete"
           ? EventType.created
           : event.type === "error"
             ? EventType.error
             : event.type === "step" && event.step === "starting"
               ? EventType.started
               : EventType.script_completed,
       message: event.message,
       metadata: JSON.stringify({ step: event.step, percent: event.percent }),
     });
   }
   ```

   Note: Map ContainerProgressEvent types to EventType enum values. Not all progress events need DB persistence — only `step` and terminal events. For `log` type events, skip DB persistence (too many rows) but still publish to Redis.

   Revise: Only persist to DB for `step`, `complete`, and `error` events. For `log` events, only publish to Redis (fire-and-forget for real-time display).

4. **5-Phase Pipeline — `processContainerCreation(job: Job<ContainerJobData, ContainerJobResult>)`:**

   **Phase 1: Create Container (0-20%)**
   - Fetch the ProxmoxNode from DB using `job.data.nodeId`
   - Create ProxmoxClient from node (`createProxmoxClientFromNode`)
   - Publish step: `{ type: "step", step: "creating", percent: 5, message: "Creating LXC container..." }`
   - Call `createContainer(client, node.name, { ... })` mapping job.data.config to `ProxmoxContainerCreateConfig`
   - The create call returns a UPID — poll with `waitForTask(client, node.name, upid, { interval: 2000, timeout: 120000 })`
   - Publish step: `{ type: "step", step: "creating", percent: 20, message: "Container created successfully" }`

   **Phase 2: Start Container (20-35%)**
   - Publish step: `{ type: "step", step: "starting", percent: 25, message: "Starting container..." }`
   - Call `startContainer(client, node.name, job.data.config.vmid)`
   - Poll UPID with `waitForTask`
   - Publish step: `{ type: "step", step: "starting", percent: 35, message: "Container started" }`

   **Phase 3: Deploy config-manager and template files via SSH (35-60%)**
   - Publish step: `{ type: "step", step: "deploying", percent: 40, message: "Connecting to container via SSH..." }`
   - Determine container IP. For DHCP: query the Proxmox API for the container's network interfaces using `client.get(\`/nodes/${node.name}/lxc/${vmid}/interfaces\`)`(may need to add this endpoint). Alternatively, if ipConfig is static (e.g.,`ip=10.0.0.50/24`), parse the IP from the config. For now, implement the static IP extraction and add a TODO for DHCP discovery.
   - Connect via SSH using `connectWithRetry({ host: containerIp, username: "root", password: job.data.config.rootPassword })` — this retries since SSH takes seconds to become available after container boot.

   **Phase 3a: Deploy config-manager infrastructure**
   - Create directory structure via SSH:
     ```
     ssh.exec("mkdir -p /etc/config-manager /etc/infrahaus/credentials /var/log/config-manager")
     ```
   - Fetch template with scripts, files, packages from DB:
     ```ts
     const template = await prisma.template.findUnique({
       where: { id: job.data.templateId },
       include: {
         scripts: { where: { enabled: true }, orderBy: { order: "asc" } },
         files: true,
         packages: true,
       },
     });
     ```
   - Write `config.env` to `/etc/config-manager/config.env` via `ssh.uploadFile()`:
     ```
     CONFIG_REPO_URL=<from env or node settings>
     CONFIG_BRANCH=main
     CONFIG_PATH=<template.path or template.name>
     TEMPLATE_NAME=<template.name>
     CONTAINER_ID=<containerId>
     ```
     Use environment variable `CONFIG_REPO_URL` from the worker process env. If template has a `path` field, use it for CONFIG_PATH; otherwise use the template name.
   - Download and install `config-sync.sh` — upload the script from the repo (`infra/lxc/scripts/config-sync.sh` if available, or generate a minimal sync script) to `/usr/local/bin/config-sync.sh` with mode `0o755`.
   - Install config-manager systemd service via `ssh.uploadFile()`:
     Write a systemd unit file to `/etc/systemd/system/config-manager.service`:

     ```ini
     [Unit]
     Description=Config Manager Sync Service
     After=network-online.target
     Wants=network-online.target

     [Service]
     Type=oneshot
     EnvironmentFile=/etc/config-manager/config.env
     ExecStart=/usr/local/bin/config-sync.sh
     StandardOutput=append:/var/log/config-manager/sync.log
     StandardError=append:/var/log/config-manager/sync.log

     [Install]
     WantedBy=multi-user.target
     ```

   - Enable the service: `ssh.exec("systemctl daemon-reload && systemctl enable config-manager.service")`
   - Publish log events for each file deployed
   - Publish step: `{ type: "step", step: "deploying", percent: 50, message: "Config-manager installed" }`

   **Phase 3b: Deploy template files**
   - Upload each template file via SSH: `ssh.uploadFile(file.content, file.targetPath + "/" + file.name, 0o644)`
   - Publish log events for each file uploaded
   - Publish step: `{ type: "step", step: "deploying", percent: 60, message: "Template files deployed" }`

   **Phase 4: Run config-manager and execute scripts (60-90%)**
   - Publish step: `{ type: "step", step: "syncing", percent: 65, message: "Running config-manager initial sync..." }`
   - Run config-manager service once: `ssh.execStreaming("systemctl start config-manager.service && journalctl -u config-manager.service --no-pager -n 50", (line, isStderr) => { publishProgress(...) })` — this triggers the config-sync.sh which handles package installation from the config.
   - Alternatively, if config-manager handles package install, skip separate package install. If template has additional scripts beyond what config-manager handles:
   - For each enabled script (sorted by order):
     - Upload script content to `/tmp/{script.name}` with mode `0o755`
     - Execute via `ssh.execStreaming("bash /tmp/{script.name}", (line, isStderr) => { publishProgress(containerId, { type: "log", message: line }) })`
     - Check exit code — if non-zero, throw error with script name
     - Publish percentage incrementally across scripts (e.g., distribute 65-90% evenly)
   - Publish step: `{ type: "step", step: "syncing", percent: 90, message: "Setup scripts completed" }`

   **Phase 5: Service discovery and finalize (90-100%)**
   - Publish step: `{ type: "step", step: "finalizing", percent: 92, message: "Discovering services..." }`

   **Phase 5a: Service and credential discovery**
   - Read credentials from `/etc/infrahaus/credentials/` if the directory has files:
     ```ts
     const credResult = await ssh.exec(
       "ls /etc/infrahaus/credentials/ 2>/dev/null || echo 'empty'",
     );
     if (credResult.stdout.trim() !== "empty") {
       // Read each credential file as JSON
       const files = credResult.stdout.trim().split("\n");
       for (const file of files) {
         const content = await ssh.exec(
           `cat /etc/infrahaus/credentials/${file}`,
         );
         // Parse and store encrypted credentials
       }
     }
     ```
   - Discover running services via systemd:
     ```ts
     const servicesResult = await ssh.exec(
       "systemctl list-units --type=service --state=running --no-pager --no-legend | awk '{print $1}'",
     );
     ```
     Filter to relevant services (exclude system services like `systemd-*`, `ssh`, `cron`, `dbus`, etc.). Keep application services.
   - Discover listening ports:
     ```ts
     const portsResult = await ssh.exec(
       "ss -tlnp 2>/dev/null | tail -n +2 | awk '{print $4, $6}'",
     );
     ```
     Parse to extract port numbers and associated process names.
   - Create ContainerService records in DB for each discovered service:
     ```ts
     await DatabaseService.createContainerService({
       containerId,
       name: serviceName,
       type: ServiceType.systemd,
       port: discoveredPort || undefined,
       webUrl: discoveredPort
         ? `http://${containerIp}:${discoveredPort}`
         : undefined,
       status: ServiceStatus.running,
       credentials: encryptedCredentials || undefined,
     });
     ```
   - Publish log events for each discovered service

   **Phase 5b: Finalize**
   - Publish step: `{ type: "step", step: "finalizing", percent: 98, message: "Finalizing container..." }`
   - Update container lifecycle to `ready`: `DatabaseService.updateContainerLifecycle(containerId, ContainerLifecycle.ready)`
   - Close SSH session
   - Publish: `{ type: "complete", percent: 100, message: "Container ready!" }`

   **Error handling:** Wrap entire pipeline in try/catch:
   - On error: update lifecycle to `error`, publish `{ type: "error", message: error.message }`, close SSH if open
   - Always close SSH in finally block
   - Return `{ success: false, containerId, vmid, error: error.message }` on failure
   - Return `{ success: true, containerId, vmid }` on success

5. **Worker instantiation:**

   ```ts
   const worker = new Worker<ContainerJobData, ContainerJobResult>(
     "container-creation",
     processContainerCreation,
     {
       connection: workerConnection,
       concurrency: 2, // Process up to 2 containers simultaneously
     },
   );

   worker.on("completed", (job, result) => {
     console.log(`Job ${job.id} completed:`, result);
   });

   worker.on("failed", (job, err) => {
     console.error(`Job ${job?.id} failed:`, err.message);
   });
   ```

6. **Graceful shutdown:**

   ```ts
   async function shutdown() {
     console.log("Shutting down worker...");
     await worker.close();
     await publisher.quit();
     await workerConnection.quit();
     process.exit(0);
   }

   process.on("SIGTERM", shutdown);
   process.on("SIGINT", shutdown);
   ```

7. **Startup log:**
   ```ts
   console.log("Container creation worker started. Waiting for jobs...");
   ```

**IP extraction helper** (used in Phase 3):

```ts
function extractIpFromConfig(ipConfig: string): string | null {
  // Handle "ip=10.0.0.50/24,gw=10.0.0.1"
  const match = ipConfig.match(/ip=(\d+\.\d+\.\d+\.\d+)/);
  return match ? match[1] : null;
}
```

If IP is "dhcp" or extraction fails, throw an error with clear message: "Cannot determine container IP. DHCP discovery not yet implemented. Use static IP in container config."
</action>
<verify> - `npx tsc --noEmit` passes in apps/dashboard - `apps/dashboard/src/workers/container-creation.ts` exists and is >150 lines - File imports from lib/queue, lib/proxmox, lib/ssh, lib/db correctly - Worker uses `maxRetriesPerRequest: null` for its Redis connection - `grep "new Worker" apps/dashboard/src/workers/container-creation.ts` finds the worker instantiation - `grep "SIGTERM" apps/dashboard/src/workers/container-creation.ts` finds graceful shutdown - `grep "config-manager" apps/dashboard/src/workers/container-creation.ts` finds config-manager deployment - `grep "createContainerService" apps/dashboard/src/workers/container-creation.ts` finds service discovery
</verify>
<done> - Worker process runs as standalone tsx script - 5-phase pipeline: Create → Start → Deploy config-manager + files (SSH) → Run config-manager + scripts (SSH) → Discover services + Finalize - Config-manager deployed: /etc/config-manager/ directory, config.env, config-sync.sh, systemd service - Progress published to Redis Pub/Sub (container:<id>:progress channel) - Step events persisted to ContainerEvent table for late subscriber replay - Log events published to Redis only (no DB persistence for high-frequency logs) - Service/credential discovery: reads /etc/infrahaus/credentials, discovers systemd services and listening ports - ContainerService records created in DB for discovered services - Container lifecycle updated: creating → ready (success) or creating → error (failure) - Graceful shutdown on SIGTERM/SIGINT - Static IP extraction from config; DHCP discovery deferred with clear error
</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` — all types valid, worker compiles
2. Worker file has correct Redis connection (maxRetriesPerRequest: null)
3. All 5 pipeline phases present with progress publishing, including config-manager deployment and service discovery
4. Error handling catches failures and transitions lifecycle to `error`
5. SSH session is always closed in finally block
6. ContainerEvent records created for step transitions
7. Config-manager directory structure, config.env, config-sync.sh, and systemd service deployed in Phase 3
8. Config-manager service runs in Phase 4 before template scripts
9. Service discovery in Phase 5 reads credentials, discovers services/ports, creates ContainerService records
10. Graceful shutdown handles SIGTERM/SIGINT
</verification>

<success_criteria>

- Worker starts via `pnpm dev:worker` (tsx --watch)
- Worker connects to Redis and waits for jobs
- On job receipt, executes 5-phase pipeline with real-time progress
- Config-manager is fully deployed: directories, config.env, sync script, systemd service
- Config-manager runs initial sync, then template scripts execute
- Services and credentials are discovered and saved as ContainerService records
- Progress events reach Redis Pub/Sub channel
- Container lifecycle transitions correctly in database
- Failed jobs produce error events and update lifecycle to error
- Worker shuts down cleanly on SIGTERM
  </success_criteria>

<output>
After completion, create `.planning/phases/03-container-creation/03-02-SUMMARY.md`
</output>
