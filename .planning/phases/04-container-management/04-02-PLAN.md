---
phase: 04-container-management
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/dashboard/src/lib/containers/monitoring.ts
autonomous: true

must_haves:
  truths:
    - "Service monitoring can SSH into a running container and check systemd service status"
    - "Port discovery uses ss to find listening TCP ports on a container"
    - "Credential reading parses /etc/infrahaus/credentials files from a container"
    - "Monitoring handles SSH connection failures gracefully without crashing"
    - "Config-manager status check detects installation state and last run info"
  artifacts:
    - path: "apps/dashboard/src/lib/containers/monitoring.ts"
      provides: "monitorContainer function, checkSystemdServices, discoverPorts, readCredentials, checkConfigManagerStatus, MonitoringResult type"
      contains: "monitorContainer"
      min_lines: 80
  key_links:
    - from: "apps/dashboard/src/lib/containers/monitoring.ts"
      to: "apps/dashboard/src/lib/ssh.ts"
      via: "Uses SSHSession to connect and run commands"
      pattern: "SSHSession|connectWithRetry"
---

<objective>
Build the service monitoring engine that SSH-es into running containers to check systemd service status, discover listening ports, read credentials, and check config-manager health. This is a pure library module — no server actions or DB writes here.

**Prerequisite: Phase 3 must be complete before executing this plan.** Phase 3 provides `SSHSession`, `connectWithRetry`, and the SSH queue infrastructure in `@/lib/ssh.ts`. This plan's monitoring.ts imports from that module.

Purpose: After container creation, the worker discovers services and saves initial ContainerService records. But services start/stop over time. This engine re-checks on demand. The actual server action that orchestrates DB updates will be added in Plan 04-04 (which depends on both this plan and 04-01).

Output: A pure monitoring library at `src/lib/containers/monitoring.ts` that takes SSH params and returns typed results.
</objective>

<execution_context>
@/home/coder/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/coder/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md

@apps/dashboard/prisma/schema.prisma
@apps/dashboard/src/lib/ssh.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Service monitoring engine</name>
  <files>
    apps/dashboard/src/lib/containers/monitoring.ts
  </files>
  <action>
Create `src/lib/containers/monitoring.ts`. Add comment `// Server-side module — do not import from client components` but NOT `import "server-only"` because the worker process from Phase 3 may also use it.

**Types:**

```ts
export interface ServiceCheckResult {
  name: string;
  active: boolean; // systemd active state
  enabled: boolean; // systemd enabled state
  subState: string; // e.g., "running", "dead", "exited"
}

export interface PortInfo {
  port: number;
  protocol: "tcp" | "tcp6";
  process: string; // process name from ss
  pid: number;
}

export interface CredentialEntry {
  service: string;
  key: string;
  value: string;
}

export interface MonitoringResult {
  services: ServiceCheckResult[];
  ports: PortInfo[];
  credentials: CredentialEntry[];
  configManagerStatus: {
    installed: boolean;
    lastRun: string | null; // ISO timestamp or null
    lastRunSuccess: boolean | null;
    lastLog: string | null; // last few lines of log
  };
  error?: string; // Set if SSH connection failed
}
```

**Functions:**

1. **`async checkSystemdServices(ssh: SSHSession, serviceNames: string[]): Promise<ServiceCheckResult[]>`**

   For each service name, check status via systemctl. Run a single combined command for efficiency:

   ```bash
   for svc in service1 service2; do
     echo "---$svc---"
     systemctl is-active "$svc" 2>/dev/null || echo "inactive"
     systemctl is-enabled "$svc" 2>/dev/null || echo "disabled"
     systemctl show "$svc" --property=SubState --value 2>/dev/null || echo "unknown"
   done
   ```

   Parse the output by splitting on `---servicename---` markers. Each marker is followed by 3 lines: active/inactive, enabled/disabled, SubState.

   If serviceNames is empty, return empty array.

2. **`async discoverPorts(ssh: SSHSession): Promise<PortInfo[]>`**

   Run: `ss -tlnp` to list listening TCP ports.
   Parse each line (skip header): extract Local Address:Port, and process/PID from the `users:` column.
   Format of `users:` column: `users:(("nginx",pid=1234,fd=6))`.
   Return array of `PortInfo`. Filter out port 22 (SSH itself).
   Handle parsing failures gracefully — if a line can't be parsed, skip it.

3. **`async readCredentials(ssh: SSHSession): Promise<CredentialEntry[]>`**

   Config-manager stores credentials in `/etc/infrahaus/credentials/` directory. Each file is named after the service (e.g., `gitea`, `portainer`). File format is simple key=value pairs per line.

   Run: `ls /etc/infrahaus/credentials/ 2>/dev/null` to list files.
   If the directory doesn't exist (exit code != 0 or empty output), return empty array.
   For each file, run: `cat /etc/infrahaus/credentials/{filename}` and parse key=value lines.
   Set `service` field to the filename (service name).

4. **`async checkConfigManagerStatus(ssh: SSHSession): Promise<MonitoringResult["configManagerStatus"]>`**

   Check config-manager status:
   - Run `systemctl is-active config-manager 2>/dev/null` — if not "active", return `{ installed: false, lastRun: null, lastRunSuccess: null, lastLog: null }`
   - If active, run `tail -20 /var/log/infrahaus/config-manager.log 2>/dev/null`
   - Parse the log for last timestamp and look for "SUCCESS" or "ERROR" markers
   - Return installed: true with parsed info

5. **`async monitorContainer(containerId: string, containerIp: string, rootPassword: string, serviceNames: string[]): Promise<MonitoringResult>`**

   Main entry point. Orchestrates all checks for a single container:
   - Try to connect via SSH with `connectWithRetry({ host: containerIp, username: "root", password: rootPassword }, { maxAttempts: 2, initialDelay: 1000 })` — only 2 attempts since container should already be running
   - If SSH connection fails, return `MonitoringResult` with `error` set to the error message and empty arrays for everything else
   - Run all 4 checks in parallel: `Promise.all([checkSystemdServices(ssh, serviceNames), discoverPorts(ssh), readCredentials(ssh), checkConfigManagerStatus(ssh)])`
   - Close SSH session in a `finally` block
   - Return combined result

   Import `connectWithRetry` from `@/lib/ssh` and `SSHSession` type.

**Exports:** `monitorContainer`, `checkSystemdServices`, `discoverPorts`, `readCredentials`, `checkConfigManagerStatus`, and all types (`ServiceCheckResult`, `PortInfo`, `CredentialEntry`, `MonitoringResult`).

  </action>
  <verify>
    - `npx tsc --noEmit` passes in apps/dashboard
    - `apps/dashboard/src/lib/containers/monitoring.ts` exists
    - Exports: monitorContainer, checkSystemdServices, discoverPorts, readCredentials, checkConfigManagerStatus
    - Exports types: ServiceCheckResult, PortInfo, CredentialEntry, MonitoringResult
    - monitorContainer handles SSH failure gracefully (returns error in result, doesn't throw)
    - No `server-only` import (worker-compatible)
  </verify>
  <done>
    - Service monitoring engine checks systemd services, discovers ports, reads credentials, checks config-manager
    - monitorContainer orchestrates all checks via SSH with graceful failure handling
    - All functions are pure (take SSH session, return typed results)
    - Module is worker-compatible (no server-only guard)
    - Types are exported for use by actions and UI
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` — all types valid
2. monitoring.ts exports monitorContainer and all check functions
3. monitorContainer handles SSH failures gracefully (no throws, returns error in result)
4. Port discovery correctly parses ss output
5. Credential reading handles missing directory
6. Config-manager status check handles missing service
</verification>

<success_criteria>

- SSH-based monitoring can check systemd services, discover ports, read credentials
- monitorContainer orchestrates all checks with graceful SSH failure handling
- All functions handle edge cases (no config-manager, no credentials dir, SSH timeout, empty service lists)
- Module is a pure library with no side effects (no DB writes, no session access)
  </success_criteria>

<output>
After completion, create `.planning/phases/04-container-management/04-02-SUMMARY.md`
</output>
